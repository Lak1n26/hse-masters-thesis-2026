{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Ranker - Ensemble of All Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.11.14 (main, Oct 28 2025, 12:11:54) [Clang 20.1.4 ]\n",
      "PyTorch version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "import cornac\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from tecd_retail_recsys.data.bert4rec_dataset import BERT4RecDatasetBuilder\n",
    "from rectools.models import BERT4RecModel, ImplicitALSWrapperModel, LightFMWrapperModel\n",
    "\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "from tecd_retail_recsys.data import DataPreprocessor\n",
    "from tecd_retail_recsys.metrics import calculate_metrics\n",
    "from tecd_retail_recsys.models import TopPopular, TopPersonal, EASE, iALS, TIFUKNN, SASRec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dlyapin/Documents/git_projects/hse-masters-thesis-2026'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13794cc50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 100\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "BERT4REC_MODEL_PATH = './models/bert4rec/bert4rec_model_exp8.pkl'\n",
    "BIVAE_MODEL_PATH = './models/bivae/bivae_model.pkl/BiVAECF/2026-02-23_23-29-23-451228.pkl'\n",
    "BPR_MODEL_PATH = './models/bpr/bpr_model.pkl/BPR/2026-02-23_16-40-10-174156.pkl'\n",
    "LIGHTFM_MODEL_PATH = './models/lightfm/lightfm_model.pkl'\n",
    "LIGHTGCN_MODEL_PATH = './models/lightgcn'\n",
    "SASREC_CHECKPOINT_PATH = './checkpoints/make_sasrec_great_again_v4_best_state.pth'\n",
    "\n",
    "FEATURES_PATH = 'all_recs_features.parquet'\n",
    "\n",
    "CATBOOST_MODEL_PATH = 'catboost_ranker_v2_model.cbm'\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "Loading events from t_ecd_small_partial/dataset/small/retail/events\n",
      "Loaded 236,479,226 total events\n",
      "Loading items data from t_ecd_small_partial/dataset/small/retail/items.pq\n",
      "Loaded 250,171 items with features: ['item_id', 'item_brand_id', 'item_category', 'item_subcategory', 'item_price', 'item_embedding']\n",
      "Merged item features. Data shape: (236479226, 12)\n",
      "Filtered to 3,758,762 events with action_type='added-to-cart'\n",
      "After filtering (min_user_interactions=1, min_item_interactions=20): 3,249,972 events, 84,944 users, 30,954 items\n",
      "Created mappings: 84944 users, 30954 items\n",
      "Temporal split - Train: days < 1269 (902,543 events), Val: days 1269-1288 (228,339 events), Test: days >= 1289 (223,395 events)\n",
      "Users in each part (train, val, test) - 7425\n",
      "Train shape: (902543, 12)\n",
      "Val shape: (228339, 12)\n",
      "Test shape: (223395, 12)\n",
      "\n",
      "Number of users: 7425\n",
      "Number of items: 30751\n"
     ]
    }
   ],
   "source": [
    "dp = DataPreprocessor(\n",
    "    day_begin=1082, \n",
    "    day_end=1308, \n",
    "    val_days=20, \n",
    "    test_days=20, \n",
    "    min_user_interactions=1, \n",
    "    min_item_interactions=20\n",
    ")\n",
    "train_df, val_df, test_df = dp.preprocess()\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Val shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "user_id_map = dp.user_to_idx\n",
    "item_id_map = dp.item_to_idx\n",
    "inv_user_id_map = {v: k for k, v in user_id_map.items()}\n",
    "inv_item_id_map = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "n_users = train_df['user_id'].nunique()\n",
    "n_items = train_df['item_id'].nunique()\n",
    "\n",
    "print(f\"\\nNumber of users: {n_users}\")\n",
    "print(f\"Number of items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation users: 7425\n",
      "Joined data shape: (7425, 4)\n"
     ]
    }
   ],
   "source": [
    "train_orig = train_df.copy()\n",
    "val_orig = val_df.copy()\n",
    "\n",
    "joined = dp.get_grouped_data(train_orig, val_orig, test_df)\n",
    "val_users = val_df['user_id'].unique()\n",
    "\n",
    "print(f\"Evaluation users: {len(val_users)}\")\n",
    "print(f\"Joined data shape: {joined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Baseline Models and Generate Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TopPopular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TopPopular...\n",
      "TopPopular recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training TopPopular...\")\n",
    "toppopular = TopPopular()\n",
    "toppopular.fit(joined, col='train_interactions')\n",
    "\n",
    "toppopular_recs_grouped = joined.copy()\n",
    "toppopular_recs_grouped['toppopular_recs'] = toppopular.predict(joined, topn=TOP_K, return_scores=False)\n",
    "toppopular_recs_grouped = toppopular_recs_grouped[['user_id', 'toppopular_recs']]\n",
    "\n",
    "print(f\"TopPopular recommendations shape: {toppopular_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TopPersonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TopPersonal...\n",
      "TopPersonal recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training TopPersonal...\")\n",
    "toppersonal = TopPersonal()\n",
    "toppersonal.fit(joined, col='train_interactions')\n",
    "\n",
    "toppersonal_recs_grouped = joined.copy()\n",
    "toppersonal_recs_grouped['toppersonal_recs'] = toppersonal.predict(joined, topn=TOP_K, return_scores=False)\n",
    "toppersonal_recs_grouped = toppersonal_recs_grouped[['user_id', 'toppersonal_recs']]\n",
    "\n",
    "print(f\"TopPersonal recommendations shape: {toppersonal_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 iALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iALS...\n",
      "Iter â„– 1/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 1976.77it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:10<00:00, 2877.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 2/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 2272.82it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:09<00:00, 3091.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 3/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 2362.19it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:14<00:00, 2072.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 4/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:06<00:00, 1138.01it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:16<00:00, 1810.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 5/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:04<00:00, 1552.02it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:14<00:00, 2168.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 6/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:04<00:00, 1647.70it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:12<00:00, 2414.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 7/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:04<00:00, 1838.40it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:12<00:00, 2559.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 8/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 2002.88it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:11<00:00, 2604.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 9/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 2077.35it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:11<00:00, 2731.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter â„– 10/10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:03<00:00, 1959.50it/s]\n",
      "Updating items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30751/30751 [00:10<00:00, 2811.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iALS recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training iALS...\")\n",
    "matrix_train, idx_to_item_train = dp.get_interactions_matrix(joined, col='train_interactions')\n",
    "ials_recs_grouped = joined.copy()\n",
    "\n",
    "ials = iALS()\n",
    "\n",
    "\n",
    "ials = iALS(idx_to_item=idx_to_item_train)\n",
    "ials.fit(matrix_train)    \n",
    "ials_recs_grouped['ials_recs'] = ials.predict(joined, topn=100)\n",
    "\n",
    "ials_recs_grouped = ials_recs_grouped[['user_id', 'ials_recs']]\n",
    "\n",
    "print(f\"iALS recommendations shape: {ials_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TIFUKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TIFUKNN...\n",
      "Building TIFU-KNN PIF vectors for 7425 users and 30751 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PIF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:01<00:00, 6500.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing user similarities and neighbors...\n",
      "Finding 1000 nearest neighbors for each user...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding neighbors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:00<00:00, 11010.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing collaborative signals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing neighbor PIF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:46<00:00, 160.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIFU-KNN training completed!\n",
      "TIFU-KNN training completed!\n",
      "Generating recommendations for 7425 users...\n",
      "Masking interacted items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:00<00:00, 37861.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing top-N recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting top-N: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7425/7425 [00:01<00:00, 4177.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIFUKNN recommendations shape: (7425, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training TIFUKNN...\")\n",
    "tifu_recs_grouped = joined.copy()\n",
    "tifu = TIFUKNN(n_neighbors=1000)\n",
    "tifu.fit(joined, col='train_interactions')\n",
    "tifu_recs_grouped['tifuknn_recs'] = tifu.predict(joined, topn=100)\n",
    "tifu_recs_grouped = tifu_recs_grouped[['user_id', 'tifuknn_recs']]\n",
    "\n",
    "print(f\"TIFUKNN recommendations shape: {tifu_recs_grouped.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Advanced Models and Generate Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ—ï¸  BERT4Rec Dataset Builder\n",
      "======================================================================\n",
      "âœ… Interactions: 902543 ÑÑ‚Ñ€Ğ¾Ğº\n",
      "ğŸ“¦ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… item features...\n",
      "  âœ… Brand: 30751 items\n",
      "  âœ… Category: 30199 items\n",
      "  âœ… Subcategory: 30199 items\n",
      "ğŸ’° Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ price features...\n",
      "  âœ… Price buckets: 30751 items, 10 categories\n",
      "  âœ… Price tier: 30751 items\n",
      "  âœ… Price in category: 29213 items\n",
      "\n",
      "ğŸ“¦ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ item features: 181864 ÑÑ‚Ñ€Ğ¾Ğº\n",
      "   Ğ¤Ğ¸Ñ‡Ğ¸: ['brand', 'category', 'subcategory', 'price_bucket', 'price_tier', 'price_in_category']\n",
      "   Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: 30751\n",
      "\n",
      "ğŸ”¨ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ RecTools Dataset...\n",
      "âœ… Dataset: 7425 users, 30751 items\n",
      "\n",
      "âœ… ItemNet: ID + Categorical\n",
      "======================================================================\n",
      "âœ… Dataset Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "builder = BERT4RecDatasetBuilder(train_df)\n",
    "dataset, item_net_config = builder.build_dataset(\n",
    "    use_item_embeddings=False,\n",
    "    use_price_features=True,\n",
    "    use_temporal_features=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT4Rec from ./models/bert4rec/bert4rec_model_exp8.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /var/folders/5t/b05_gxx17hnftz_n3c82pt4h0000gn/T/tmp0jm29xmk\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 9.4 M  | train\n",
      "-----------------------------------------------------------------\n",
      "9.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.4 M     Total params\n",
      "37.799    Total estimated model params size (MB)\n",
      "40        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /var/folders/5t/b05_gxx17hnftz_n3c82pt4h0000gn/T/tmp0jm29xmk\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT4Rec recommendations shape: (742500, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Loading BERT4Rec from {BERT4REC_MODEL_PATH}...\")\n",
    "\n",
    "\n",
    "bert4rec_model = BERT4RecModel.load(BERT4REC_MODEL_PATH)\n",
    "\n",
    "bert4rec_recs = bert4rec_model.recommend(\n",
    "    users=val_users,\n",
    "    dataset=dataset,\n",
    "    k=TOP_K,\n",
    "    filter_viewed=False,\n",
    "    on_unsupported_targets=\"ignore\"\n",
    ")\n",
    "\n",
    "bert4rec_recs_grouped = bert4rec_recs.groupby('user_id', as_index=False)['item_id'].agg(list)\n",
    "bert4rec_recs_grouped.columns = ['user_id', 'bert4rec_recs']\n",
    "\n",
    "print(f\"BERT4Rec recommendations shape: {bert4rec_recs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 BiVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BiVAE from ./models/bivae/bivae_model.pkl/BiVAECF/2026-02-23_23-29-23-451228.pkl...\n",
      "BiVAE recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Loading BiVAE from {BIVAE_MODEL_PATH}...\")\n",
    "\n",
    "train_agg = train_df.rename(columns={'user_id': 'userID', 'item_id': 'itemID'}).groupby(['userID', 'itemID']).size().reset_index(name='count')\n",
    "train_data = [(str(row['userID']), str(row['itemID']), float(row['count'])) \n",
    "              for _, row in train_agg.iterrows()]\n",
    "train_set = cornac.data.Dataset.from_uir(train_data, seed=SEED)\n",
    "\n",
    "\n",
    "bivae_model = cornac.models.BiVAECF.load(BIVAE_MODEL_PATH)\n",
    "bivae_model.train_set = train_set\n",
    "\n",
    "bivae_recs = predict_ranking(bivae_model, train_set, usercol='userID', itemcol='itemID', remove_seen=False)\n",
    "bivae_recs_top100 = (bivae_recs\n",
    "          .sort_values(['userID', 'prediction'], ascending=[True, False])\n",
    "          .groupby('userID', as_index=False)\n",
    "          .head(100)\n",
    "         )\n",
    "bivae_recs_top100['userID'] = bivae_recs_top100['userID'].astype(np.int64)\n",
    "bivae_recs_top100['itemID'] = bivae_recs_top100['itemID'].astype(np.int64)\n",
    "\n",
    "bivae_recs_grouped = bivae_recs_top100.groupby('userID', as_index=False).itemID.agg(list)\n",
    "bivae_recs_grouped.columns = ['user_id', 'bivae_recs']\n",
    "\n",
    "print(f\"BiVAE recommendations shape: {bivae_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BPR from ./models/bpr/bpr_model.pkl/BPR/2026-02-23_16-40-10-174156.pkl...\n",
      "BPR recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading BPR from {BPR_MODEL_PATH}...\")\n",
    "\n",
    "bpr_model = cornac.models.BPR.load(BPR_MODEL_PATH)\n",
    "bpr_model.train_set = train_set\n",
    "\n",
    "bpr_recs = predict_ranking(bpr_model, train_set, usercol='userID', itemcol='itemID', remove_seen=False)\n",
    "bpr_recs_top100 = (bpr_recs\n",
    "          .sort_values(['userID', 'prediction'], ascending=[True, False])\n",
    "          .groupby('userID', as_index=False)\n",
    "          .head(100)\n",
    "         )\n",
    "bpr_recs_top100['userID'] = bpr_recs_top100['userID'].astype(np.int64)\n",
    "bpr_recs_top100['itemID'] = bpr_recs_top100['itemID'].astype(np.int64)\n",
    "\n",
    "bpr_recs_grouped = bpr_recs_top100.groupby('userID', as_index=False).itemID.agg(list)\n",
    "bpr_recs_grouped.columns = ['user_id', 'bpr_recs']\n",
    "\n",
    "print(f\"BPR recommendations shape: {bpr_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LightFM from ./models/lightfm/lightfm_model.pkl...\n",
      "\n",
      "======================================================================\n",
      "ğŸ—ï¸  BERT4Rec Dataset Builder\n",
      "======================================================================\n",
      "âœ… Interactions: 902543 ÑÑ‚Ñ€Ğ¾Ğº\n",
      "ğŸ“¦ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… item features...\n",
      "  âœ… Brand: 30751 items\n",
      "  âœ… Category: 30199 items\n",
      "  âœ… Subcategory: 30199 items\n",
      "ğŸ‘¤ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ user features...\n",
      "  âœ… Interaction level: 7425 users\n",
      "  âœ… User price segment: 7425 users\n",
      "  âœ… Favorite brand: 7425 users\n",
      "  âœ… Favorite category: 7423 users\n",
      "  âœ… User diversity: 7423 users\n",
      "  âœ… User recency: 7425 users\n",
      "\n",
      "ğŸ“¦ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ item features: 91149 ÑÑ‚Ñ€Ğ¾Ğº\n",
      "   Ğ¤Ğ¸Ñ‡Ğ¸: ['brand', 'category', 'subcategory']\n",
      "   Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: 30751\n",
      "\n",
      "ğŸ‘¤ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ user features: 44546 ÑÑ‚Ñ€Ğ¾Ğº\n",
      "   Ğ¤Ğ¸Ñ‡Ğ¸: ['interaction_level', 'user_price_segment', 'favorite_brand', 'favorite_category', 'user_diversity', 'user_recency']\n",
      "   Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: 7425\n",
      "\n",
      "ğŸ”¨ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ RecTools Dataset...\n",
      "âœ… Dataset: 7425 users, 30751 items\n",
      "\n",
      "âœ… ItemNet: ID + Categorical\n",
      "======================================================================\n",
      "âœ… Dataset Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ!\n",
      "======================================================================\n",
      "\n",
      "LightFM recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading LightFM from {LIGHTFM_MODEL_PATH}...\")\n",
    "\n",
    "builder = BERT4RecDatasetBuilder(train_df)\n",
    "dataset, item_net_config = builder.build_dataset(\n",
    "    use_item_embeddings=False,\n",
    "    use_price_features=False,\n",
    "    use_temporal_features=False,\n",
    "    use_user_features=True,\n",
    "    n_factors=512\n",
    ")\n",
    "\n",
    "lightfm_model = LightFMWrapperModel.load(LIGHTFM_MODEL_PATH)\n",
    "\n",
    "lightfm_recs = lightfm_model.recommend(\n",
    "    users=val_users,\n",
    "    dataset=dataset,\n",
    "    k=TOP_K,\n",
    "    filter_viewed=False,\n",
    "    on_unsupported_targets=\"ignore\"\n",
    ")\n",
    "\n",
    "lightfm_recs_grouped = lightfm_recs.groupby('user_id', as_index=False)['item_id'].agg(list)\n",
    "lightfm_recs_grouped.columns = ['user_id', 'lightfm_recs']\n",
    "\n",
    "print(f\"LightFM recommendations shape: {lightfm_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SASRec from ./checkpoints/make_sasrec_great_again_v4_best_state.pth...\n",
      "SASRec recommendations shape: (7425, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading SASRec from {SASREC_CHECKPOINT_PATH}...\")\n",
    "\n",
    "sasrec_model = SASRec(\n",
    "    checkpoint_path=SASREC_CHECKPOINT_PATH,\n",
    "    max_seq_len=100,\n",
    "    device='cpu',\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "sasrec_model.load_model()\n",
    "\n",
    "sasrec_recs = sasrec_model.predict(joined, topn=TOP_K, return_scores=False)\n",
    "sasrec_recs_grouped = joined.copy()\n",
    "sasrec_recs_grouped['sasrec_recs'] = sasrec_recs\n",
    "sasrec_recs_grouped = sasrec_recs_grouped[['user_id', 'sasrec_recs']]\n",
    "\n",
    "print(f\"SASRec recommendations shape: {sasrec_recs_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge All Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All recommendations merged. Shape: (7425, 13)\n",
      "Columns: ['user_id', 'train_interactions', 'val_interactions', 'test_interactions', 'toppopular_recs', 'toppersonal_recs', 'ials_recs', 'tifuknn_recs', 'bert4rec_recs', 'bivae_recs', 'bpr_recs', 'lightfm_recs', 'sasrec_recs']\n"
     ]
    }
   ],
   "source": [
    "# Merge all recommendations\n",
    "all_recs = joined.copy()\n",
    "\n",
    "for recs_df, col_name in [\n",
    "    (toppopular_recs_grouped, 'toppopular_recs'),\n",
    "    (toppersonal_recs_grouped, 'toppersonal_recs'),\n",
    "    (ials_recs_grouped, 'ials_recs'),\n",
    "    (tifu_recs_grouped, 'tifuknn_recs'),\n",
    "    (bert4rec_recs_grouped, 'bert4rec_recs'),\n",
    "    (bivae_recs_grouped, 'bivae_recs'),\n",
    "    (bpr_recs_grouped, 'bpr_recs'),\n",
    "    (lightfm_recs_grouped, 'lightfm_recs'),\n",
    "    (sasrec_recs_grouped, 'sasrec_recs')\n",
    "]:\n",
    "    all_recs = all_recs.merge(recs_df, on='user_id', how='left')\n",
    "    all_recs[col_name] = all_recs[col_name].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "\n",
    "print(f\"All recommendations merged. Shape: {all_recs.shape}\")\n",
    "print(f\"Columns: {all_recs.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_recs.to_parquet(\"all_recs.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Combined Candidate Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of unique candidates per user: 558.6\n",
      "Min candidates: 433\n",
      "Max candidates: 729\n"
     ]
    }
   ],
   "source": [
    "def combine_candidates(row):\n",
    "    \"\"\"Combine all model recommendations for a user, removing duplicates.\"\"\"\n",
    "    candidates = set()\n",
    "    \n",
    "    for col in ['toppopular_recs', 'toppersonal_recs', \n",
    "                'ials_recs',  'tifuknn_recs', \n",
    "                'bert4rec_recs', 'bivae_recs', 'bpr_recs', \n",
    "                'lightfm_recs', 'sasrec_recs'\n",
    "                ]:\n",
    "        if isinstance(row[col], list):\n",
    "            candidates.update(row[col])\n",
    "    \n",
    "    return list(candidates)\n",
    "\n",
    "all_recs['combined_candidates'] = all_recs.apply(combine_candidates, axis=1)\n",
    "all_recs['num_candidates'] = all_recs['combined_candidates'].apply(len)\n",
    "\n",
    "print(f\"Average number of unique candidates per user: {all_recs['num_candidates'].mean():.1f}\")\n",
    "print(f\"Min candidates: {all_recs['num_candidates'].min()}\")\n",
    "print(f\"Max candidates: {all_recs['num_candidates'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Features and Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data with features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd490d96b184d42939724b8bbe9c7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating pairs:   0%|          | 0/7425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training pairs created: 4147617\n",
      "Positive samples: 540973\n",
      "Negative samples: 3606644\n",
      "Positive rate: 0.1304\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating training data with features...\")\n",
    "\n",
    "training_pairs = []\n",
    "\n",
    "for idx, row in tqdm(all_recs.iterrows(), total=len(all_recs), desc=\"Creating pairs\"):\n",
    "    user_id = row['user_id']\n",
    "    candidates = row['combined_candidates']\n",
    "    val_items = [item_id for item_id, _, _ in row['train_interactions']]\n",
    "    \n",
    "    for item_id in candidates:\n",
    "        target = 1 if item_id in val_items else 0\n",
    "        training_pairs.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'target': target\n",
    "        })\n",
    "\n",
    "training_df_pd = pd.DataFrame(training_pairs)\n",
    "print(f\"\\nTraining pairs created: {len(training_df_pd)}\")\n",
    "print(f\"Positive samples: {(training_df_pd['target'] == 1).sum()}\")\n",
    "print(f\"Negative samples: {(training_df_pd['target'] == 0).sum()}\")\n",
    "print(f\"Positive rate: {(training_df_pd['target'] == 1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df_pd.to_parquet(\"training_df_pd.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_features: (7425, 14), nans: 0\n",
      "item_features: (30954, 19), nans: 0\n",
      "brand_features: (5413, 6), nans: 0\n",
      "user_item_features: (630733, 9), nans: 0\n",
      "user_brand_features: (33989, 12), nans: 0\n",
      "user_category_features: (27837, 12), nans: 0\n",
      "done! nans:  7447868\n"
     ]
    }
   ],
   "source": [
    "# collect features with pairs (user_id, item_id)\n",
    "\n",
    "from tecd_retail_recsys.data.features import add_features_to_samples\n",
    "\n",
    "features = add_features_to_samples(samples_df=training_df_pd, train_df=train_df, dp=dp)\n",
    "features.drop(columns=['item_brand_embedding', 'item_embedding', 'brand_embedding'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_parquet(FEATURES_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to polars\n",
    "del features\n",
    "features_df = pl.read_parquet(FEATURES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding candidate generator ranks as features...\n",
      "Processing toppopular_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing toppersonal_recs...\n",
      "  Created 443926 user-item-rank mappings\n",
      "Processing ials_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing tifuknn_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing bert4rec_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing bivae_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing bpr_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing lightfm_recs...\n",
      "  Created 742500 user-item-rank mappings\n",
      "Processing sasrec_recs...\n",
      "  Created 742500 user-item-rank mappings\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding candidate generator ranks as features...\")\n",
    "\n",
    "all_recs = pd.read_parquet(\"all_recs.parquet\")\n",
    "\n",
    "model_columns = [\n",
    "    'toppopular_recs', 'toppersonal_recs', 'ials_recs', \n",
    "    'tifuknn_recs', 'bert4rec_recs', 'bivae_recs', 'bpr_recs', \n",
    "    'lightfm_recs', 'sasrec_recs'\n",
    "]\n",
    "\n",
    "for col in model_columns:\n",
    "    if col in all_recs.columns:\n",
    "        if all_recs[col].dtype == 'object':\n",
    "            all_recs[col] = all_recs[col].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "            )\n",
    "\n",
    "\n",
    "rank_features = {}\n",
    "\n",
    "for model_col in model_columns:\n",
    "    print(f\"Processing {model_col}...\")\n",
    "    model_name = model_col.replace('_recs', '')\n",
    "    \n",
    "    user_item_to_rank = {}\n",
    "    \n",
    "    for idx, row in all_recs.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        recs = row[model_col].tolist()\n",
    "        \n",
    "        if isinstance(recs, list) and len(recs) > 0:\n",
    "            for rank, item_id in enumerate(recs, start=1):\n",
    "                user_item_to_rank[(user_id, item_id)] = rank\n",
    "    \n",
    "    rank_features[model_name] = user_item_to_rank\n",
    "    print(f\"  Created {len(user_item_to_rank)} user-item-rank mappings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data after joining features: (4147617, 64)\n",
      "Missing features: shape: (1, 64)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ user_id â”† item_id â”† target â”† item_brand_ â”† â€¦ â”† user_categ â”† user_categ â”† user_categ â”† user_categ â”‚\n",
      "â”‚ ---     â”† ---     â”† ---    â”† id          â”†   â”† ory_days_s â”† ory_days_s â”† ory_lifeti â”† ory_orders â”‚\n",
      "â”‚ u32     â”† u32     â”† u32    â”† ---         â”†   â”† ince_first â”† ince_last_ â”† me         â”† _share     â”‚\n",
      "â”‚         â”†         â”†        â”† u32         â”†   â”† â€¦          â”† â€¦          â”† ---        â”† ---        â”‚\n",
      "â”‚         â”†         â”†        â”†             â”†   â”† ---        â”† ---        â”† u32        â”† u32        â”‚\n",
      "â”‚         â”†         â”†        â”†             â”†   â”† u32        â”† u32        â”†            â”†            â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0       â”† 0       â”† 0      â”† 0           â”† â€¦ â”† 0          â”† 0          â”† 170408     â”† 170408     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ nulls\n"
     ]
    }
   ],
   "source": [
    "training_df_pl = pl.from_pandas(training_df_pd)\n",
    "\n",
    "training_data = training_df_pl.join(\n",
    "    features_df,\n",
    "    on=['user_id', 'item_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Training data after joining features: {training_data.shape}\")\n",
    "print(f\"Missing features: {training_data.null_count().sum()[0]} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns to fill: 55\n",
      "Computed medians for 52 columns\n",
      "Filled numeric columns with median values\n",
      "Categorical columns to fill: 5\n",
      "After filling nulls: 0 nulls remaining\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = [col for col in training_data.columns if training_data[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]]\n",
    "print(f\"Numeric columns to fill: {len(numeric_cols)}\")\n",
    "\n",
    "medians = {}\n",
    "for col in numeric_cols:\n",
    "    if col not in ['user_id', 'item_id', 'target']:\n",
    "        median_val = training_data[col].median()\n",
    "        medians[col] = median_val if median_val is not None else 0.0\n",
    "\n",
    "print(f\"Computed medians for {len(medians)} columns\")\n",
    "\n",
    "fill_expressions = [\n",
    "    pl.col(col).fill_null(medians[col]) \n",
    "    for col in medians.keys()\n",
    "]\n",
    "\n",
    "if fill_expressions:\n",
    "    training_data = training_data.with_columns(fill_expressions)\n",
    "\n",
    "print(f\"Filled numeric columns with median values\")\n",
    "\n",
    "categorical_cols = [col for col in training_data.columns if training_data[col].dtype == pl.Utf8]\n",
    "print(f\"Categorical columns to fill: {len(categorical_cols)}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    training_data = training_data.with_columns([\n",
    "        pl.col(col).fill_null('unknown') for col in categorical_cols\n",
    "    ])\n",
    "\n",
    "print(f\"After filling nulls: {training_data.null_count().to_numpy().sum()} nulls remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Features for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['user_id', 'item_id', 'item_brand_id', 'user_os', 'user_main_subdomain', 'user_socdem_cluster', 'user_region', 'item_category', 'item_subcategory']\n",
      "  Added rank_toppopular: min=1, max=101, mean=91.96\n",
      "  Added rank_toppersonal: min=1, max=101, mean=94.59\n",
      "  Added rank_ials: min=1, max=101, mean=91.96\n",
      "  Added rank_tifuknn: min=1, max=101, mean=91.96\n",
      "  Added rank_bert4rec: min=1, max=101, mean=91.96\n",
      "  Added rank_bivae: min=1, max=101, mean=91.96\n",
      "  Added rank_bpr: min=1, max=101, mean=91.96\n",
      "  Added rank_lightfm: min=1, max=101, mean=91.96\n",
      "  Added rank_sasrec: min=1, max=101, mean=91.96\n",
      "\n",
      "Final training data:\n",
      "X shape: (4147617, 72)\n",
      "y shape: (4147617,)\n",
      "Number of unique users (groups): 7425\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'user_id', 'item_id', 'item_brand_id', 'user_os', 'user_main_subdomain',\n",
    "    'user_socdem_cluster', 'user_region', 'item_category', 'item_subcategory',\n",
    "]\n",
    "\n",
    "categorical_features = [col for col in categorical_features if col in training_data.columns]\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "training_data_pd = training_data.to_pandas()\n",
    "\n",
    "for model_name, user_item_rank_map in rank_features.items():\n",
    "    rank_col_name = f'rank_{model_name}'\n",
    "    \n",
    "    ranks = []\n",
    "    for idx, row in training_data_pd.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        \n",
    "        rank = user_item_rank_map.get((user_id, item_id), 101)\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    training_data_pd[rank_col_name] = ranks\n",
    "    print(f\"  Added {rank_col_name}: min={min(ranks)}, max={max(ranks)}, mean={np.mean(ranks):.2f}\")\n",
    "\n",
    "X = training_data_pd.drop('target', axis=1)\n",
    "y = training_data_pd['target']\n",
    "groups = training_data_pd['user_id']\n",
    "\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype(str)\n",
    "    \n",
    "print(f\"\\nFinal training data:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of unique users (groups): {groups.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train CatBoost Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_drop_cols = [\n",
    "    'user_item_added_to_cart_total',\n",
    "    'user_item_days_since_first_purchase',\n",
    "    'user_item_days_since_last_purchase',\n",
    "    'user_item_top_subdomain',\n",
    "    'user_item_viewed_total',\n",
    "    'rank_toppersonal'\n",
    "    ]\n",
    "\n",
    "X = X.drop(columns=need_to_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra feature engineering\n",
    "\n",
    "ranks_cols = X.columns[X.columns.str.startswith('rank_')].tolist()\n",
    "\n",
    "X['rank_sum'] = X[ranks_cols].sum(axis=1)\n",
    "X['rank_mean'] = X[ranks_cols].mean(axis=1)\n",
    "X['rank_min'] = X[ranks_cols].min(axis=1)\n",
    "X['rank_max'] = X[ranks_cols].max(axis=1)\n",
    "X['rank_std'] = X[ranks_cols].std(axis=1)\n",
    "X['rank_median'] = X[ranks_cols].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3730750 samples, 6682 users\n",
      "Val set: 416867 samples, 743 users\n",
      "Train positive rate: 0.1303\n",
      "Val positive rate: 0.1318\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "unique_users = groups.unique()\n",
    "train_users = np.random.choice(unique_users, size=int(TRAIN_RATIO * len(unique_users)), replace=False)\n",
    "val_users_cb = np.setdiff1d(unique_users, train_users)\n",
    "\n",
    "train_mask = groups.isin(train_users)\n",
    "val_mask = groups.isin(val_users_cb)\n",
    "\n",
    "X_train, y_train, groups_train = X[train_mask], y[train_mask], groups[train_mask]\n",
    "X_val, y_val, groups_val = X[val_mask], y[val_mask], groups[val_mask]\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples, {len(train_users)} users\")\n",
    "print(f\"Val set: {X_val.shape[0]} samples, {len(val_users_cb)} users\")\n",
    "print(f\"Train positive rate: {y_train.mean():.4f}\")\n",
    "print(f\"Val positive rate: {y_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost pools created successfully\n"
     ]
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_id=groups_train,\n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_val,\n",
    "    label=y_val,\n",
    "    group_id=groups_val,\n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "print(\"CatBoost pools created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost Ranker...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266e9b12071847f3a0604cd795356bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupwise loss function. OneHotMaxSize set to 10\n",
      "0:\ttest: 0.7711051\tbest: 0.7711051 (0)\ttotal: 2.75s\tremaining: 45m 44s\n",
      "10:\ttest: 0.8954846\tbest: 0.8979850 (5)\ttotal: 29.2s\tremaining: 43m 47s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.8979850396\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training CatBoost Ranker...\")\n",
    "\n",
    "model = CatBoostRanker(\n",
    "    iterations=1000,\n",
    "    loss_function='YetiRank:top=100',\n",
    "    custom_metric=['NDCG:top=100', 'PrecisionAt:top=100', 'RecallAt:top=100'],\n",
    "    eval_metric='NDCG:top=100',\n",
    "    random_seed=RANDOM_SEED,\n",
    "    verbose=10,\n",
    "    early_stopping_rounds=10,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>rank_lightfm</td>\n",
       "      <td>53.604064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>rank_ials</td>\n",
       "      <td>11.944269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>rank_mean</td>\n",
       "      <td>11.435989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rank_min</td>\n",
       "      <td>6.567093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>rank_bpr</td>\n",
       "      <td>3.983447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>user_item_lifetime</td>\n",
       "      <td>3.291610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>user_item_clicked_total</td>\n",
       "      <td>2.670923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>rank_tifuknn</td>\n",
       "      <td>2.533861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>item_uniq_users_7d</td>\n",
       "      <td>2.507330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>user_items</td>\n",
       "      <td>1.461412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>user_brand_days_since_last_order</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>user_category_orders</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>user_brand_rank</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>user_brand_total_share</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>user_brand_lifetime</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>user_brand_days_since_first_order</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>user_category_min_price</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>user_brand_std_price</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>user_brand_max_price</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>user_brand_min_price</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 cols        imp\n",
       "64                       rank_lightfm  53.604064\n",
       "59                          rank_ials  11.944269\n",
       "67                          rank_mean  11.435989\n",
       "68                           rank_min   6.567093\n",
       "63                           rank_bpr   3.983447\n",
       "36                 user_item_lifetime   3.291610\n",
       "37            user_item_clicked_total   2.670923\n",
       "60                       rank_tifuknn   2.533861\n",
       "30                 item_uniq_users_7d   2.507330\n",
       "10                         user_items   1.461412\n",
       "44   user_brand_days_since_last_order   0.000000\n",
       "48               user_category_orders   0.000000\n",
       "47                    user_brand_rank   0.000000\n",
       "46             user_brand_total_share   0.000000\n",
       "45                user_brand_lifetime   0.000000\n",
       "43  user_brand_days_since_first_order   0.000000\n",
       "50            user_category_min_price   0.000000\n",
       "42               user_brand_std_price   0.000000\n",
       "41               user_brand_max_price   0.000000\n",
       "40               user_brand_min_price   0.000000"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'cols': X.columns, 'imp': model.get_feature_importance(type='PredictionValuesChange')}).sort_values('imp', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "CatBoost recommendations generated for 7425 users\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions for validation set...\")\n",
    "\n",
    "val_predictions = model.predict(X)\n",
    "training_data_pd['catboost_score'] = val_predictions\n",
    "\n",
    "catboost_recs = training_data_pd.sort_values(['user_id', 'catboost_score'], ascending=[True, False])\n",
    "catboost_recs_grouped = catboost_recs.groupby('user_id').head(100)\n",
    "catboost_recs_grouped = catboost_recs_grouped.groupby('user_id', as_index=False)['item_id'].agg(list)\n",
    "catboost_recs_grouped.columns = ['user_id', 'catboost_recs']\n",
    "\n",
    "print(f\"CatBoost recommendations generated for {len(catboost_recs_grouped)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataframe shape: (7425, 5)\n",
      "Users with CatBoost recommendations: 7425\n"
     ]
    }
   ],
   "source": [
    "evaluation_df = joined.merge(catboost_recs_grouped, on='user_id', how='left')\n",
    "evaluation_df['catboost_recs'] = evaluation_df['catboost_recs'].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "print(f\"Evaluation dataframe shape: {evaluation_df.shape}\")\n",
    "print(f\"Users with CatBoost recommendations: {(evaluation_df['catboost_recs'].str.len() > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics debug] resolved gt_col='val_interactions' item_id_index=0\n",
      "[Metrics debug] ratings_true shape: (228339, 3) ratings_pred shape: (742500, 3)\n",
      "  ratings_true dtypes: {'user_id': dtype('int64'), 'item_id': dtype('int64')}\n",
      "  ratings_pred dtypes: {'user_id': dtype('int64'), 'item_id': dtype('int64')}\n",
      "  user_id=11 gt_count=22 pred_count=100 overlap=7\n",
      "  user_id=14 gt_count=5 pred_count=100 overlap=0\n",
      "    [ID spaces] gt sample=[9341, 16732, 17585, 28024, 30789] range=[9341, 30789] | rec sample=[311, 560, 958, 1809, 2175] range=[311, 30251]\n",
      "  user_id=21 gt_count=47 pred_count=100 overlap=16\n",
      "\n",
      "At k=10:\n",
      "  MAP@10       = 0.1259\n",
      "  NDCG@10      = 0.3110\n",
      "  Precision@10 = 0.1774\n",
      "  Recall@10    = 0.0594\n",
      "\n",
      "At k=100:\n",
      "  MAP@100       = 0.0829\n",
      "  NDCG@100      = 0.2655\n",
      "  Precision@100 = 0.0777\n",
      "  Recall@100    = 0.2148\n",
      "\n",
      "Other Metrics:\n",
      "  MRR                 = 0.2166\n",
      "  Catalog Coverage    = 0.9822\n",
      "  Diversity     = 0.9967  [0=same recs for all, 1=unique recs]\n",
      "  Novelty             = 0.9411\n",
      "  Serendipity         = 0.0181\n"
     ]
    }
   ],
   "source": [
    "metrics_result = calculate_metrics(\n",
    "    evaluation_df,\n",
    "    train_col='train_interactions',\n",
    "    gt_col='val_interactions',\n",
    "    model_preds='catboost_recs',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to catboost_ranker_v2_model.cbm\n",
      "Recommendations saved to catboost_v2_val_recs.parquet\n"
     ]
    }
   ],
   "source": [
    "model.save_model(CATBOOST_MODEL_PATH)\n",
    "print(f\"Model saved to {CATBOOST_MODEL_PATH}\")\n",
    "\n",
    "catboost_recs_grouped.to_parquet('catboost_v2_val_recs.parquet', index=False)\n",
    "print(\"Recommendations saved to catboost_v2_val_recs.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ĞĞ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ ndcg@100 = 0.2655`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
